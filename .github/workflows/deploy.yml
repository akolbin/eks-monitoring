name: Deploy to EKS

on:
  workflow_run:
    workflows: ["Build and Test"]
    types:
      - completed
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'prod'
        type: choice
        options:
        - dev
        - staging
        - prod
      force_deploy:
        description: 'Force deployment even if tests failed'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: us-east-1

permissions:
  contents: read
  security-events: write
  actions: read
  id-token: write

jobs:
  deploy-infrastructure:
    permissions:
      contents: read
      id-token: write
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event.inputs.force_deploy == 'true' }}

    strategy:
      matrix:
        environment: ${{ github.event.inputs.environment && fromJson(format('["{0}"]', github.event.inputs.environment)) || fromJson('["prod"]') }}

    environment: ${{ matrix.environment }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@main
      with:
        role-to-assume: arn:aws:iam::836072596305:role/GitHub_Actions
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: Terraform Init
      run: |
        cd terraform
        terraform init \
          -backend-config="bucket=akolbin-eks-monitoring-terraform-state" \
          -backend-config="key=terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \

    - name: Terraform Plan
      run: |
        cd terraform
        terraform plan -var-file="environments/${{ matrix.environment }}.tfvars" -out=tfplan

    - name: Terraform Apply
      run: |
        cd terraform
        terraform apply -auto-approve tfplan

    - name: Get EKS cluster info
      id: eks-info
      run: |
        cd terraform
        echo "cluster_name=$(terraform output -raw cluster_id)" >> $GITHUB_OUTPUT
        echo "cluster_endpoint=$(terraform output -raw cluster_endpoint)" >> $GITHUB_OUTPUT

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ steps.eks-info.outputs.cluster_name }}

    - name: Verify cluster access
      run: |
        kubectl get nodes
        kubectl get namespaces

    outputs:
      cluster_name: ${{ steps.eks-info.outputs.cluster_name }}
      cluster_endpoint: ${{ steps.eks-info.outputs.cluster_endpoint }}

  deploy-monitoring:
    name: Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: deploy-infrastructure

    strategy:
      matrix:
        environment: ${{ github.event.inputs.environment && fromJson(format('["{0}"]', github.event.inputs.environment)) || fromJson('["dev"]') }}

    environment: ${{ matrix.environment }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@main
      with:
        role-to-assume: arn:aws:iam::836072596305:role/GitHub_Actions
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.13.0'

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ needs.deploy-infrastructure.outputs.cluster_name }}

    - name: Create namespaces
      run: |
        kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
        kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -

    - name: Add Helm repositories
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo add fluent https://fluent.github.io/helm-charts
        helm repo update

    - name: Deploy monitoring stack
      run: |
        cd helm/monitoring-stack
        helm dependency update

        # Deploy with environment-specific values
        helm upgrade --install monitoring-stack . \
          --namespace monitoring \
          --values values.yaml \
          --values values-${{ matrix.environment }}.yaml \
          --wait \
          --timeout 10m

    - name: Verify deployment
      run: |
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus --namespace monitoring --timeout=300s
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=grafana --namespace monitoring --timeout=300s
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=fluent-bit --namespace logging --timeout=300s

        # Check deployment status
        kubectl get pods -n monitoring
        kubectl get pods -n logging
        kubectl get services -n monitoring

  post-deployment-tests:
    name: Post-Deployment Tests
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, deploy-monitoring]

    strategy:
      matrix:
        environment: ${{ github.event.inputs.environment && fromJson(format('["{0}"]', github.event.inputs.environment)) || fromJson('["dev"]') }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@main
      with:
        role-to-assume: arn:aws:iam::836072596305:role/GitHub_Actions
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ needs.deploy-infrastructure.outputs.cluster_name }}

    - name: Test Prometheus metrics
      run: |
        # Port forward to Prometheus
        kubectl port-forward -n monitoring svc/monitoring-stack-kube-prom-prometheus 9090:9090 &
        sleep 10

        # Test metrics endpoint
        curl -f http://localhost:9090/api/v1/query?query=up || exit 1

        # Kill port-forward
        pkill -f "kubectl port-forward"

    - name: Test Grafana accessibility
      run: |
        # Port forward to Grafana
        kubectl port-forward -n monitoring svc/monitoring-stack-grafana 3000:80 &
        sleep 10

        # Test Grafana health
        curl -f http://localhost:3000/api/health || exit 1

        # Kill port-forward
        pkill -f "kubectl port-forward"

    - name: Test Fluent Bit logs
      run: |
        # Check if Fluent Bit is collecting logs
        kubectl logs -n logging -l app.kubernetes.io/name=fluent-bit --tail=50

    - name: Validate monitoring stack health
      run: |
        # Check all pods are running
        kubectl get pods -n monitoring -o wide
        kubectl get pods -n logging -o wide

        # Check for any failed pods
        FAILED_PODS=$(kubectl get pods -n monitoring --field-selector=status.phase!=Running --no-headers | wc -l)
        if [ $FAILED_PODS -gt 0 ]; then
          echo "Found failed pods in monitoring namespace"
          kubectl get pods -n monitoring --field-selector=status.phase!=Running
          exit 1
        fi

        FAILED_PODS=$(kubectl get pods -n logging --field-selector=status.phase!=Running --no-headers | wc -l)
        if [ $FAILED_PODS -gt 0 ]; then
          echo "Found failed pods in logging namespace"
          kubectl get pods -n logging --field-selector=status.phase!=Running
          exit 1
        fi

  notify:
    name: Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, deploy-monitoring, post-deployment-tests]
    if: always()

    steps:
    - name: Notify success
      if: needs.post-deployment-tests.result == 'success'
      run: |
        echo "✅ Deployment to ${{ matrix.environment }} completed successfully!"
        # Add Slack/Teams notification here if needed

    - name: Notify failure
      if: needs.post-deployment-tests.result == 'failure'
      run: |
        echo "❌ Deployment to ${{ matrix.environment }} failed!"
        # Add Slack/Teams notification here if needed
